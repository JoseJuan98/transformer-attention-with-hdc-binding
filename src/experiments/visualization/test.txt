Traditional Transformers integrate positional information via additive \glspl{pe} \cite{vaswani2017attention}, a simple superposition method with known limitations such as anisotropy in the embedding space \cite{liang2021isotropic}, which can decrease performance on sequential data like time series. This thesis investigates alternative approaches to enhance Transformer efficacy for \gls{tsc} by leveraging \gls{hdc} binding operations, specifically component-wise multiplication and circular convolution, and by exploring \glspl{pe} with explicitly designed similarity shapes. This research is further motivated by recent advancements in \glspl{llm}, such as the development of novel \glspl{pe} like \gls{rope} \cite{su2024roformer}, time series specific encodings like \gls{tape} \cite{foumani2024convtran}, adaptive encodings that learn optimal frequencies \cite{sun2024adaptive_sin}, and attention mechanisms like \gls{mla} \cite{deepseekai2024deepseekv2}, which also move beyond simple additive integration to embed positional information more structurally.

Using an encoder-only Transformer architecture and a custom experimental framework, various configurations were systematically evaluated on a diverse set of UCR/UEA \acrlong{tsc} datasets \cite{dau2019ucr}. The experiments compared \gls{hdc} binding methods against the standard additive approach, using both linear and 1D convolutional input embeddings, and evaluated multiple \gls{pe} schemes including sinusoidal, split sinusoidal, random, \gls{fpe}, and no \gls{pe}.
% TODO: add about the other experiments
A final experiment compares the best performing configuration against other modern techniques, including \gls{rope}, ConvTran \cite{foumani2024convtran} and \gls{mla}, to contextualize the findings within the current \gls{sota}.

% TODO: change the numbers to the most recent results
Key findings demonstrate that \gls{hdc} binding methods, especially circular convolution, significantly outperform the conventional additive approach. For instance, with a single encoder layer, circular convolution combined with 1D convolutional embeddings achieved a mean test accuracy of XX.YY\%, substantially higher than XX.YY\% for additive binding with the same inputs. Furthermore, \glspl{fpe}, designed for gradual similarity decay between positions, surpassed standard sinusoidal \glspl{pe} (e.g., XX.YY\% vs. XX.YY\% with circular convolution binding and linear embeddings). \gls{hdc} methods maintained their superiority in deeper models, although the relative performance gain over depth did not consistently increase.

These results strongly suggest that moving beyond simple additive \gls{pe} integration and carefully designing \gls{pe} similarity profiles can substantially improve Transformer performance on \gls{tsc} tasks, offering promising avenues for future architectural enhancements.

Code and experiment results are open-sourced at:
\url{https://github.com/JoseJuan98/transformer-attention-with-hdc-binding}
